{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD, Adam\n",
    "from AlexNet import AlexNet\n",
    "from helper import load_tiny_imagenet\n",
    "from helper import evaluate_accuracy, evaluate_loss\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "# Download: http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "train, val, test = load_tiny_imagenet().values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "alexNet_sgd = AlexNet(100, verbose=True)\n",
    "alexNet_adam = AlexNet(100, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We trained our models using stochastic gradient descent with a batch size of 128 examples,\n",
    "    momentum of 0.9, and weight decay of 0.0005.\n",
    "We used an equal learning rate for all layers, which we adjusted manually throughout training.\n",
    "The heuristic which we followed was to divide the learning rate by 10 when the validation\n",
    "    error rate stopped improving with the current learning rate.\n",
    "The learning rate was initialized at 0.01 and reduced three times prior to termination.\n",
    "\"\"\"\n",
    "LR = 0.01\n",
    "p = 0.9\n",
    "decay = 0.0005\n",
    "\n",
    "# Original paper uses SGD as the optimizer, which doesn't converge=...\n",
    "sgd_optimizer = SGD(alexNet_sgd.parameters(), lr=LR, momentum=p, weight_decay=decay)\n",
    "adam_optimizer = Adam(alexNet_adam.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler_sgd = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=sgd_optimizer,\n",
    "        mode='min', factor=0.1, patience=5, threshold=0.002\n",
    ")\n",
    "\n",
    "scheduler_adam = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=adam_optimizer,\n",
    "        mode='min', factor=0.1, patience=5, threshold=1e-4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alexNet_sgd.initialize(optimizer=sgd_optimizer, scheduler=scheduler_sgd)\n",
    "alexNet_sgd.train(mode=True, data=train, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1, batch 200] loss: 7.135173358917236\n",
      "[epoch 1, batch 400] loss: 4.607536435127258\n",
      "[epoch 1, batch 600] loss: 4.60124048948288\n",
      "[epoch 1, batch 800] loss: 4.597749187946319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-1b92a7b2cbbe>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0malexNet_adam\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minitialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0madam_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mscheduler_adam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0malexNet_adam\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\git\\DeepLearningPlayground\\AlexNet.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, mode, data, epochs)\u001B[0m\n\u001B[0;32m    173\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m                 \u001B[0mrunning_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    176\u001B[0m                 \u001B[0mbatch_split\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m                 \u001B[0mbatch_split\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mbatch_split\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mbatch_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "alexNet_adam.initialize(optimizer=adam_optimizer, scheduler=scheduler_adam)\n",
    "alexNet_adam.train(mode=True, data=train, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2e9ef25e",
   "language": "python",
   "display_name": "PyCharm (DeepLearningPlayground)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}